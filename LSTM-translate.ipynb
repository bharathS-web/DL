{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e90242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "english_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I love deep learning.\",\n",
    "    \"What is your name?\",\n",
    "    \"Where do you live?\"\n",
    "]\n",
    "\n",
    "tamil_sentences = [\n",
    "    \"வணக்கம், நீங்கள் எப்படி இருக்கிறீர்கள்?\",\n",
    "    \"எனக்கு ஆழ்ந்த கற்றல் பிடிக்கும்.\",\n",
    "    \"உங்கள் பெயர் என்ன?\",\n",
    "    \"நீங்கள் எங்கு வாழ்கிறீர்கள்?\"\n",
    "]\n",
    "\n",
    "# Tokenize English sentences\n",
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(english_sentences)\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_sequences = eng_tokenizer.texts_to_sequences(english_sentences)\n",
    "\n",
    "# Tokenize Tamil sentences\n",
    "tam_tokenizer = Tokenizer()\n",
    "tam_tokenizer.fit_on_texts(tamil_sentences)\n",
    "tam_vocab_size = len(tam_tokenizer.word_index) + 1\n",
    "tam_sequences = tam_tokenizer.texts_to_sequences(tamil_sentences)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_length = max(max(len(seq) for seq in eng_sequences), max(len(seq) for seq in tam_sequences))\n",
    "eng_sequences = pad_sequences(eng_sequences, maxlen=max_length, padding='post')\n",
    "tam_sequences = pad_sequences(tam_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Split input-output pairs\n",
    "X_train = eng_sequences\n",
    "y_train = tam_sequences\n",
    "\n",
    "# Define Seq2Seq Model (Encoder-Decoder)\n",
    "embedding_dim = 128\n",
    "units = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = keras.layers.Input(shape=(max_length,))\n",
    "enc_emb = keras.layers.Embedding(eng_vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm = keras.layers.LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = keras.layers.Input(shape=(max_length,))\n",
    "dec_emb = keras.layers.Embedding(tam_vocab_size, embedding_dim, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = keras.layers.LSTM(units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(tam_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Compile Model\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Shift decoder input (y_train) correctly\n",
    "decoder_input_data = y_train[:, :-1]  # Remove last token\n",
    "decoder_target_data = y_train[:, 1:]  # Remove first token\n",
    "\n",
    "# Ensure shapes match\n",
    "print(\"Encoder Input Shape:\", X_train.shape)  # (num_samples, max_length)\n",
    "print(\"Decoder Input Shape:\", decoder_input_data.shape)  # (num_samples, max_length - 1)\n",
    "print(\"Decoder Target Shape:\", decoder_target_data.shape)  # (num_samples, max_length - 1)\n",
    "\n",
    "# Fix shape mismatch by adjusting max_length\n",
    "decoder_input_data = pad_sequences(decoder_input_data, maxlen=max_length, padding='post')\n",
    "decoder_target_data = pad_sequences(decoder_target_data, maxlen=max_length, padding='post')\n",
    "\n",
    "# Train Model\n",
    "model.fit([X_train, decoder_input_data], decoder_target_data, batch_size=64, epochs=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7723c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate English → Tamil\n",
    "def translate(sentence):\n",
    "    sequence = eng_tokenizer.texts_to_sequences([sentence])\n",
    "    sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    states_value = model.predict([sequence, np.zeros((1, max_length))])\n",
    "    predicted_seq = np.argmax(states_value, axis=-1)[0]\n",
    "    output_sentence = ' '.join([word for word, index in tam_tokenizer.word_index.items() if index in predicted_seq])\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5174c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(\"what is  name\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
