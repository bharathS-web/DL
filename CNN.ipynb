{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 images belonging to 6 classes.\n",
      "Found 12 images belonging to 6 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.2083 - loss: 0.6201 - val_accuracy: 0.1667 - val_loss: 0.6046\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.1389 - loss: 0.5271 - val_accuracy: 0.1667 - val_loss: 0.4657\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.1944 - loss: 0.4657 - val_accuracy: 0.2500 - val_loss: 0.4742\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.4236 - loss: 0.4405 - val_accuracy: 0.3333 - val_loss: 0.4647\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 904ms/step - accuracy: 0.3611 - loss: 0.4094 - val_accuracy: 0.2500 - val_loss: 0.4581\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.5347 - loss: 0.3608 - val_accuracy: 0.1667 - val_loss: 0.4419\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.8056 - loss: 0.2876 - val_accuracy: 0.0833 - val_loss: 0.4819\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.7778 - loss: 0.2594 - val_accuracy: 0.2500 - val_loss: 0.5275\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.9375 - loss: 0.2082 - val_accuracy: 0.2500 - val_loss: 0.5606\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.9514 - loss: 0.1541 - val_accuracy: 0.1667 - val_loss: 0.7228\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step - accuracy: 0.9861 - loss: 0.0954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10159042477607727, 0.9791666865348816]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = r\"D:\\intel-small\"\n",
    "# Data Preprocessing\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(250, 250),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",  # Ensures multi-class training\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "\n",
    "# Load Validation Data\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(250, 250),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# Define CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(10, (3,3), activation='relu', input_shape=(250, 250, 3)),\n",
    "    Conv2D(10, (3,3), activation='relu'),\n",
    "    \n",
    "    Conv2D(35, (3,3), activation='relu'),\n",
    "    Conv2D(35, (3,3), activation='relu'),\n",
    "    MaxPooling2D((5,5)),  # 10x10x35\n",
    "    \n",
    "    Conv2D(100, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),  # 2x2x100\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(train_generator.num_classes, activation='softmax')  # Use \"softmax\" for multi-class\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # Change to categorical_crossentropy if multi-class\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=10)\n",
    "\n",
    "# Save Model\n",
    "# model.save(\"cnn_model.h5\")\n",
    "\n",
    "model.evaluate(train_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "Predicted class: street\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Path to the new image\n",
    "new_image_path = r\"D:\\Intel-Image-Data\\seg_test\\seg_test\\street\\20126.jpg\"\n",
    "new_image_path = r\"D:\\Intel-Image-Data\\seg_train\\seg_train\\forest\\175.jpg\"\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = image.load_img(new_image_path, target_size=(250, 250))  # Resize to match model input\n",
    "img_array = image.img_to_array(img) / 255.0  # Normalize\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "\n",
    "# Predict class probabilities\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# Get the class with the highest probability\n",
    "predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "# Load class labels\n",
    "class_labels = {v: k for k, v in train_generator.class_indices.items()}  # Reverse mapping\n",
    "\n",
    "# Print the predicted class\n",
    "print(\"Predicted class:\", class_labels[predicted_class])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50, EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "# Load ResNet50 (pretrained on ImageNet)\n",
    "base_model_resnet = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(250, 250, 3))\n",
    "\n",
    "# Freeze base model layers\n",
    "base_model_resnet.trainable = False\n",
    "\n",
    "# Add custom classification head\n",
    "x = GlobalAveragePooling2D()(base_model_resnet.output)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Create model\n",
    "resnet_model = Model(inputs=base_model_resnet.input, outputs=output)\n",
    "\n",
    "# Compile model\n",
    "resnet_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model\n",
    "resnet_model.fit(train_generator, validation_data=val_generator, epochs=10)\n",
    "\n",
    "# Save model\n",
    "# resnet_model.save(\"resnet_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### efficient net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EfficientNetB0 (pretrained on ImageNet)\n",
    "base_model_efficientnet = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(250, 250, 3))\n",
    "\n",
    "# Freeze base model layers\n",
    "base_model_efficientnet.trainable = False\n",
    "\n",
    "# Add custom classification head\n",
    "x = GlobalAveragePooling2D()(base_model_efficientnet.output)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Create model\n",
    "efficientnet_model = Model(inputs=base_model_efficientnet.input, outputs=output)\n",
    "\n",
    "# Compile model\n",
    "efficientnet_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model\n",
    "efficientnet_model.fit(train_generator, validation_data=val_generator, epochs=10)\n",
    "\n",
    "# Save model\n",
    "# efficientnet_model.save(\"efficientnet_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ultralytics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load YOLOv8 (pretrained on COCO dataset)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8s.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 's' = small, 'm' = medium, 'l' = large\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 (pretrained on COCO dataset)\n",
    "model = YOLO(\"yolov8s.pt\")  # 's' = small, 'm' = medium, 'l' = large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform object detection\n",
    "results = model(\"path_to_your_image.jpg\", save=True)\n",
    "\n",
    "# Show detection results\n",
    "results[0].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run YOLO on a video\n",
    "results = model(\"path_to_video.mp4\", save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run YOLO on webcam\n",
    "model.predict(source=0, show=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
