{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and Preprocess Data\n",
    "file_path = r\"Dataset\\\\en.txt\"\n",
    "\n",
    "# Open the file and read the sentences\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    sentences = file.readlines()\n",
    "\n",
    "# Clean the sentences (remove newline characters and extra spaces)\n",
    "sentences = [sentence.strip() for sentence in sentences]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1223596"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sentences[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to sequences\n",
    "input_sequences = []\n",
    "for line in dataset:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[191, 223],\n",
       " [191, 223, 265],\n",
       " [5, 13],\n",
       " [5, 13, 3],\n",
       " [5, 13, 3, 39],\n",
       " [5, 13, 3, 39, 3],\n",
       " [5, 13, 3, 39, 3, 324],\n",
       " [302, 6],\n",
       " [302, 6, 2002],\n",
       " [302, 6, 2002, 3612],\n",
       " [302, 6, 2002, 3612, 16],\n",
       " [302, 6, 2002, 3612, 16, 11],\n",
       " [302, 6, 2002, 3612, 16, 11, 6],\n",
       " [302, 6, 2002, 3612, 16, 11, 6, 3613],\n",
       " [302, 6, 2002, 3612, 16, 11, 6, 3613, 1017],\n",
       " [2003, 6],\n",
       " [2003, 6, 1438],\n",
       " [2003, 6, 1438, 115],\n",
       " [1, 2589],\n",
       " [1, 2589, 6],\n",
       " [1, 2589, 6, 2003],\n",
       " [5, 25],\n",
       " [5, 25, 19],\n",
       " [5, 25, 19, 141],\n",
       " [5, 25, 19, 141, 118],\n",
       " [57, 22],\n",
       " [57, 22, 4],\n",
       " [57, 22, 4, 931],\n",
       " [57, 22, 4, 931, 14],\n",
       " [57, 22, 4, 931, 14, 374],\n",
       " [45, 6],\n",
       " [45, 6, 139],\n",
       " [45, 6, 139, 88],\n",
       " [45, 6, 139, 88, 3],\n",
       " [45, 6, 139, 88, 3, 266],\n",
       " [5, 128],\n",
       " [5, 128, 34],\n",
       " [5, 128, 34, 81],\n",
       " [5, 128, 34, 81, 26],\n",
       " [5, 128, 34, 81, 26, 3],\n",
       " [5, 128, 34, 81, 26, 3, 101],\n",
       " [17, 18],\n",
       " [17, 18, 47],\n",
       " [17, 18, 47, 1258],\n",
       " [17, 18, 47, 1258, 3614],\n",
       " [5, 18],\n",
       " [5, 18, 8],\n",
       " [5, 18, 8, 1],\n",
       " [5, 18, 8, 1, 1134],\n",
       " [6, 11],\n",
       " [6, 11, 4],\n",
       " [6, 11, 4, 751],\n",
       " [6, 11, 4, 751, 456],\n",
       " [5, 34],\n",
       " [5, 34, 81],\n",
       " [5, 34, 81, 55],\n",
       " [5, 34, 81, 55, 5],\n",
       " [5, 34, 81, 55, 5, 13],\n",
       " [5, 34, 81, 55, 5, 13, 1],\n",
       " [5, 34, 81, 55, 5, 13, 1, 48],\n",
       " [325, 8],\n",
       " [325, 8, 45],\n",
       " [325, 8, 45, 187],\n",
       " [325, 8, 45, 187, 3615],\n",
       " [325, 8, 45, 187, 3615, 30],\n",
       " [119, 8],\n",
       " [119, 8, 74],\n",
       " [119, 8, 74, 1135],\n",
       " [119, 8, 74, 1135, 83],\n",
       " [119, 8, 74, 1135, 83, 5],\n",
       " [119, 8, 74, 1135, 83, 5, 121],\n",
       " [2, 12],\n",
       " [2, 12, 8],\n",
       " [2, 12, 8, 24],\n",
       " [2, 12, 8, 24, 95],\n",
       " [45, 25],\n",
       " [45, 25, 600],\n",
       " [45, 25, 600, 3616],\n",
       " [5, 63],\n",
       " [5, 63, 3617],\n",
       " [5, 63, 3617, 4],\n",
       " [5, 63, 3617, 4, 129],\n",
       " [5, 75],\n",
       " [5, 75, 106],\n",
       " [5, 75, 106, 35],\n",
       " [5, 75, 106, 35, 118],\n",
       " [5, 75, 106, 35, 118, 16],\n",
       " [5, 75, 106, 35, 118, 16, 128],\n",
       " [5, 75, 106, 35, 118, 16, 128, 3618],\n",
       " [5, 75, 106, 35, 118, 16, 128, 3618, 752],\n",
       " [60, 149],\n",
       " [60, 149, 2],\n",
       " [60, 149, 2, 34],\n",
       " [60, 149, 2, 34, 96],\n",
       " [60, 149, 2, 34, 96, 3],\n",
       " [60, 149, 2, 34, 96, 3, 19],\n",
       " [60, 149, 2, 34, 96, 3, 19, 539],\n",
       " [17, 279],\n",
       " [17, 279, 707],\n",
       " [601, 29],\n",
       " [601, 29, 36],\n",
       " [601, 29, 36, 19],\n",
       " [601, 29, 36, 19, 4],\n",
       " [601, 29, 36, 19, 4, 386],\n",
       " [601, 29, 36, 19, 4, 386, 1691],\n",
       " [111, 21],\n",
       " [111, 21, 24],\n",
       " [111, 21, 24, 161],\n",
       " [111, 21, 24, 161, 20],\n",
       " [111, 21, 24, 161, 20, 3],\n",
       " [111, 21, 24, 161, 20, 3, 3619],\n",
       " [111, 21, 24, 161, 20, 3, 3619, 10],\n",
       " [111, 21, 24, 161, 20, 3, 3619, 10, 602],\n",
       " [5, 36],\n",
       " [5, 36, 109],\n",
       " [5, 36, 109, 561],\n",
       " [5, 36, 109, 561, 55],\n",
       " [5, 36, 109, 561, 55, 45],\n",
       " [5, 36, 109, 561, 55, 45, 6],\n",
       " [5, 36, 109, 561, 55, 45, 6, 1],\n",
       " [5, 36, 109, 561, 55, 45, 6, 1, 286],\n",
       " [5, 36, 109, 561, 55, 45, 6, 1, 286, 14],\n",
       " [5, 36, 109, 561, 55, 45, 6, 1, 286, 14, 387],\n",
       " [5, 36, 109, 561, 55, 45, 6, 1, 286, 14, 387, 603],\n",
       " [5, 1439],\n",
       " [5, 1439, 60],\n",
       " [5, 1439, 60, 483],\n",
       " [5, 1439, 60, 483, 41],\n",
       " [5, 1439, 60, 483, 41, 2],\n",
       " [5, 1439, 60, 483, 41, 2, 92],\n",
       " [5, 1439, 60, 483, 41, 2, 92, 59],\n",
       " [5, 1439, 60, 483, 41, 2, 92, 59, 11],\n",
       " [5, 1439, 60, 483, 41, 2, 92, 59, 11, 112],\n",
       " [5, 1439, 60, 483, 41, 2, 92, 59, 11, 112, 1],\n",
       " [5, 1439, 60, 483, 41, 2, 92, 59, 11, 112, 1, 114],\n",
       " [5, 1439, 60, 483, 41, 2, 92, 59, 11, 112, 1, 114, 932],\n",
       " [5, 314],\n",
       " [5, 314, 2],\n",
       " [111, 224],\n",
       " [111, 224, 171],\n",
       " [111, 224, 171, 353],\n",
       " [111, 224, 171, 353, 41],\n",
       " [111, 224, 171, 353, 41, 5],\n",
       " [111, 224, 171, 353, 41, 5, 64],\n",
       " [111, 224, 171, 353, 41, 5, 64, 141],\n",
       " [5, 125],\n",
       " [5, 125, 1259],\n",
       " [5, 125, 1259, 2590],\n",
       " [5, 125, 1259, 2590, 3620],\n",
       " [5, 125, 1259, 2590, 3620, 70],\n",
       " [2, 42],\n",
       " [2, 42, 324],\n",
       " [57, 88],\n",
       " [57, 88, 3],\n",
       " [57, 88, 3, 39],\n",
       " [5, 273],\n",
       " [5, 273, 171],\n",
       " [5, 273, 171, 3],\n",
       " [5, 273, 171, 3, 654],\n",
       " [5, 273, 171, 3, 654, 30],\n",
       " [5, 273, 171, 3, 654, 30, 415],\n",
       " [5, 273, 171, 3, 654, 30, 415, 753],\n",
       " [119, 71],\n",
       " [119, 71, 1440],\n",
       " [119, 71, 1440, 23],\n",
       " [119, 71, 1440, 23, 30],\n",
       " [5, 102],\n",
       " [5, 102, 167],\n",
       " [5, 102, 167, 17],\n",
       " [5, 102, 167, 17, 506],\n",
       " [5, 102, 167, 17, 506, 7],\n",
       " [5, 102, 167, 17, 506, 7, 120],\n",
       " [5, 188],\n",
       " [5, 188, 604],\n",
       " [5, 188, 604, 3],\n",
       " [5, 188, 604, 3, 19],\n",
       " [5, 188, 604, 3, 19, 47],\n",
       " [5, 188, 604, 3, 19, 47, 3621],\n",
       " [5, 139],\n",
       " [5, 139, 1259],\n",
       " [5, 139, 1259, 3622],\n",
       " [1, 123],\n",
       " [1, 123, 335],\n",
       " [1, 123, 335, 5],\n",
       " [1, 123, 335, 5, 273],\n",
       " [1, 123, 335, 5, 273, 24],\n",
       " [1, 123, 335, 5, 273, 24, 231],\n",
       " [1, 123, 335, 5, 273, 24, 231, 3],\n",
       " [1, 123, 335, 5, 273, 24, 231, 3, 388],\n",
       " [1, 123, 335, 5, 273, 24, 231, 3, 388, 5],\n",
       " [1, 123, 335, 5, 273, 24, 231, 3, 388, 5, 18],\n",
       " [1, 123, 335, 5, 273, 24, 231, 3, 388, 5, 18, 3623],\n",
       " [55, 1],\n",
       " [55, 1, 187],\n",
       " [55, 1, 187, 3624],\n",
       " [55, 1, 187, 3624, 8],\n",
       " [55, 1, 187, 3624, 8, 1],\n",
       " [55, 1, 187, 3624, 8, 1, 1135],\n",
       " [55, 1, 187, 3624, 8, 1, 1135, 11],\n",
       " [55, 1, 187, 3624, 8, 1, 1135, 11, 6],\n",
       " [55, 1, 187, 3624, 8, 1, 1135, 11, 6, 115],\n",
       " [55, 1, 187, 3624, 8, 1, 1135, 11, 6, 115, 5],\n",
       " [55, 1, 187, 3624, 8, 1, 1135, 11, 6, 115, 5, 104],\n",
       " [55, 1, 187, 3624, 8, 1, 1135, 11, 6, 115, 5, 104, 1018],\n",
       " [55, 1, 187, 3624, 8, 1, 1135, 11, 6, 115, 5, 104, 1018, 655],\n",
       " [11, 6],\n",
       " [11, 6, 3625],\n",
       " [11, 6, 3625, 225],\n",
       " [91, 12],\n",
       " [91, 12, 82],\n",
       " [91, 12, 82, 562],\n",
       " [91, 12, 82, 562, 2004],\n",
       " [91, 12, 82, 562, 2004, 204],\n",
       " [91, 12, 82, 562, 2004, 204, 165],\n",
       " [91, 12, 82, 562, 2004, 204, 165, 168],\n",
       " [91, 12, 82, 562, 2004, 204, 165, 168, 3],\n",
       " [91, 12, 82, 562, 2004, 204, 165, 168, 3, 354],\n",
       " [91, 12, 82, 562, 2004, 204, 165, 168, 3, 354, 14],\n",
       " [91, 12, 82, 562, 2004, 204, 165, 168, 3, 354, 14, 804],\n",
       " [91, 12, 82, 562, 2004, 204, 165, 168, 3, 354, 14, 804, 2591],\n",
       " [200, 68],\n",
       " [200, 68, 92],\n",
       " [200, 68, 92, 57],\n",
       " [200, 68, 92, 57, 933],\n",
       " [53, 57],\n",
       " [53, 57, 20],\n",
       " [53, 57, 20, 2],\n",
       " [53, 57, 20, 2, 12],\n",
       " [540, 24],\n",
       " [540, 24, 605],\n",
       " [457, 2592],\n",
       " [457, 2592, 30],\n",
       " [12, 2],\n",
       " [12, 2, 219],\n",
       " [484, 31],\n",
       " [484, 31, 6],\n",
       " [484, 31, 6, 4],\n",
       " [484, 31, 6, 4, 210],\n",
       " [1692, 267],\n",
       " [1692, 267, 4],\n",
       " [1692, 267, 4, 2593],\n",
       " [140, 35],\n",
       " [11, 315],\n",
       " [11, 315, 232],\n",
       " [11, 315, 232, 30],\n",
       " [14, 72],\n",
       " [14, 72, 606],\n",
       " [14, 72, 606, 5],\n",
       " [14, 72, 606, 5, 192],\n",
       " [14, 72, 606, 5, 192, 70],\n",
       " [14, 72, 606, 5, 192, 70, 708],\n",
       " [14, 72, 606, 5, 192, 70, 708, 22],\n",
       " [14, 72, 606, 5, 192, 70, 708, 22, 291],\n",
       " [11, 709],\n",
       " [11, 709, 15],\n",
       " [11, 709, 15, 1],\n",
       " [11, 709, 15, 1, 2594],\n",
       " [12, 2],\n",
       " [12, 2, 3626],\n",
       " [12, 2, 3626, 3627],\n",
       " [12, 2, 3626, 3627, 30],\n",
       " [540, 1],\n",
       " [540, 1, 3628],\n",
       " [540, 1, 3628, 253],\n",
       " [540, 1, 3628, 253, 247],\n",
       " [540, 1, 3628, 253, 247, 292],\n",
       " [540, 1, 3628, 253, 247, 292, 213],\n",
       " [5, 34],\n",
       " [5, 34, 96],\n",
       " [5, 34, 96, 3],\n",
       " [5, 34, 96, 3, 19],\n",
       " [5, 34, 96, 3, 19, 3629],\n",
       " [5, 34, 96, 3, 19, 3629, 5],\n",
       " [5, 34, 96, 3, 19, 3629, 5, 96],\n",
       " [5, 34, 96, 3, 19, 3629, 5, 96, 3],\n",
       " [5, 34, 96, 3, 19, 3629, 5, 96, 3, 19],\n",
       " [5, 34, 96, 3, 19, 3629, 5, 96, 3, 19, 2005],\n",
       " [41, 5],\n",
       " [41, 5, 710],\n",
       " [41, 5, 710, 35],\n",
       " [41, 5, 710, 35, 5],\n",
       " [41, 5, 710, 35, 5, 96],\n",
       " [41, 5, 710, 35, 5, 96, 3],\n",
       " [41, 5, 710, 35, 5, 96, 3, 19],\n",
       " [41, 5, 710, 35, 5, 96, 3, 19, 4],\n",
       " [41, 5, 710, 35, 5, 96, 3, 19, 4, 1260],\n",
       " [1136, 6],\n",
       " [1136, 6, 4],\n",
       " [1136, 6, 4, 1137],\n",
       " [1136, 6, 4, 1137, 193],\n",
       " [1136, 6, 4, 1137, 193, 3],\n",
       " [1136, 6, 4, 1137, 193, 3, 19],\n",
       " [1136, 6, 4, 1137, 193, 3, 19, 55],\n",
       " [1136, 6, 4, 1137, 193, 3, 19, 55, 2],\n",
       " [1136, 6, 4, 1137, 193, 3, 19, 55, 2, 12],\n",
       " [1136, 6, 4, 1137, 193, 3, 19, 55, 2, 12, 107],\n",
       " [1136, 6, 4, 1137, 193, 3, 19, 55, 2, 12, 107, 3],\n",
       " [1136, 6, 4, 1137, 193, 3, 19, 55, 2, 12, 107, 3, 3630],\n",
       " [1136, 6, 4, 1137, 193, 3, 19, 55, 2, 12, 107, 3, 3630, 73],\n",
       " [57, 71],\n",
       " [57, 71, 1261],\n",
       " [71, 26],\n",
       " [57, 1262],\n",
       " [57, 1262, 2006],\n",
       " [57, 1262, 2006, 44],\n",
       " [57, 20],\n",
       " [57, 20, 4],\n",
       " [57, 20, 4, 607],\n",
       " [57, 20, 4, 607, 226],\n",
       " [57, 20, 4, 607, 226, 57],\n",
       " [57, 20, 4, 607, 226, 57, 128],\n",
       " [57, 20, 4, 607, 226, 57, 128, 4],\n",
       " [57, 20, 4, 607, 226, 57, 128, 4, 1441],\n",
       " [57, 20, 4, 607, 226, 57, 128, 4, 1441, 3631],\n",
       " [57, 128],\n",
       " [57, 128, 608],\n",
       " [17, 18],\n",
       " [17, 18, 1442],\n",
       " [17, 18, 1442, 26],\n",
       " [17, 18, 1442, 26, 1443],\n",
       " [17, 18, 1442, 26, 1443, 122],\n",
       " [17, 18, 1442, 26, 1443, 122, 805],\n",
       " [247, 125],\n",
       " [247, 125, 1693],\n",
       " [247, 125, 1693, 26],\n",
       " [247, 125, 1693, 26, 2595],\n",
       " [247, 125, 1693, 26, 2595, 19],\n",
       " [247, 125, 1693, 26, 2595, 19, 50],\n",
       " [247, 125, 1693, 26, 2595, 19, 50, 3],\n",
       " [247, 125, 1693, 26, 2595, 19, 50, 3, 13],\n",
       " [247, 125, 1693, 26, 2595, 19, 50, 3, 13, 2596],\n",
       " [45, 6],\n",
       " [45, 6, 26],\n",
       " [45, 6, 26, 5],\n",
       " [45, 6, 26, 5, 66],\n",
       " [45, 6, 26, 5, 66, 13],\n",
       " [45, 6, 26, 5, 66, 13, 213],\n",
       " [11, 66],\n",
       " [11, 66, 61],\n",
       " [11, 66, 61, 2007],\n",
       " [11, 66, 61, 2007, 14],\n",
       " [11, 66, 61, 2007, 14, 30],\n",
       " [11, 66, 61, 2007, 14, 30, 3],\n",
       " [11, 66, 61, 2007, 14, 30, 3, 754],\n",
       " [11, 66, 61, 2007, 14, 30, 3, 754, 211],\n",
       " [540, 149],\n",
       " [540, 149, 119],\n",
       " [540, 149, 119, 4],\n",
       " [540, 149, 119, 4, 609],\n",
       " [601, 5],\n",
       " [601, 5, 102],\n",
       " [601, 5, 102, 142],\n",
       " [601, 5, 102, 142, 1694],\n",
       " [601, 5, 102, 142, 1694, 1695],\n",
       " [60, 4],\n",
       " [60, 4, 458],\n",
       " [60, 4, 458, 274],\n",
       " [60, 4, 458, 274, 50],\n",
       " [60, 4, 458, 274, 50, 3],\n",
       " [60, 4, 458, 274, 50, 3, 214],\n",
       " [60, 4, 458, 274, 50, 3, 214, 4],\n",
       " [60, 4, 458, 274, 50, 3, 214, 4, 2008],\n",
       " [60, 4, 458, 274, 50, 3, 214, 4, 2008, 14],\n",
       " [11, 66],\n",
       " [11, 66, 19],\n",
       " [11, 66, 19, 265],\n",
       " [11, 66, 19, 265, 274],\n",
       " [11, 66, 19, 265, 274, 13],\n",
       " [11, 66, 19, 265, 274, 13, 3],\n",
       " [11, 66, 19, 265, 274, 13, 3, 1444],\n",
       " [5, 34],\n",
       " [5, 34, 2009],\n",
       " [5, 34, 2009, 3],\n",
       " [5, 34, 2009, 3, 19],\n",
       " [5, 34, 2009, 3, 19, 2010],\n",
       " [191, 1263],\n",
       " [191, 1263, 1],\n",
       " [191, 1263, 1, 806],\n",
       " [191, 1263, 1, 806, 17],\n",
       " [191, 1263, 1, 806, 17, 104],\n",
       " [191, 1263, 1, 806, 17, 104, 707],\n",
       " [54, 79],\n",
       " [54, 79, 485],\n",
       " [54, 79, 485, 254],\n",
       " [54, 79, 485, 254, 21],\n",
       " [54, 79, 485, 254, 21, 2],\n",
       " [54, 79, 485, 254, 21, 2, 13],\n",
       " [5, 75],\n",
       " [5, 75, 19],\n",
       " [5, 75, 19, 3632],\n",
       " [5, 75, 19, 3632, 56],\n",
       " [5, 75, 19, 3632, 56, 11],\n",
       " [5, 75, 19, 3632, 56, 11, 315],\n",
       " [5, 75, 19, 3632, 56, 11, 315, 416],\n",
       " [5, 75, 19, 3632, 56, 11, 315, 416, 5],\n",
       " [5, 75, 19, 3632, 56, 11, 315, 416, 5, 34],\n",
       " [5, 75, 19, 3632, 56, 11, 315, 416, 5, 34, 326],\n",
       " [5, 75, 19, 3632, 56, 11, 315, 416, 5, 34, 326, 3],\n",
       " [5, 75, 19, 3632, 56, 11, 315, 416, 5, 34, 326, 3, 68],\n",
       " [45, 6],\n",
       " [45, 6, 125],\n",
       " [45, 6, 125, 1],\n",
       " [45, 6, 125, 1, 95],\n",
       " [45, 6, 125, 1, 95, 11],\n",
       " [45, 6, 125, 1, 95, 11, 51],\n",
       " [45, 6, 125, 1, 95, 11, 51, 90],\n",
       " [5, 92],\n",
       " [5, 92, 11],\n",
       " [5, 92, 11, 6],\n",
       " [5, 92, 11, 6, 161],\n",
       " [5, 92, 11, 6, 161, 20],\n",
       " [5, 92, 11, 6, 161, 20, 3],\n",
       " [5, 92, 11, 6, 161, 20, 3, 19],\n",
       " [5, 92, 11, 6, 161, 20, 3, 19, 3633],\n",
       " [69, 36],\n",
       " [69, 36, 125],\n",
       " [69, 36, 125, 214],\n",
       " [69, 36, 125, 214, 48],\n",
       " [274, 19],\n",
       " [274, 19, 2011],\n",
       " [274, 19, 2011, 56],\n",
       " [274, 19, 2011, 56, 5],\n",
       " [274, 19, 2011, 56, 5, 1019],\n",
       " [274, 19, 2011, 56, 5, 1019, 1264],\n",
       " [274, 19, 2011, 56, 5, 1019, 1264, 541],\n",
       " [141, 8],\n",
       " [141, 8, 344],\n",
       " [141, 8, 344, 52],\n",
       " [141, 8, 344, 52, 5],\n",
       " [141, 8, 344, 52, 5, 116],\n",
       " [141, 8, 344, 52, 5, 116, 35],\n",
       " [141, 8, 344, 52, 5, 116, 35, 22],\n",
       " [141, 8, 344, 52, 5, 116, 35, 22, 1265],\n",
       " [141, 8, 344, 52, 5, 116, 35, 22, 1265, 4],\n",
       " [141, 8, 344, 52, 5, 116, 35, 22, 1265, 4, 807],\n",
       " [141, 8, 344, 52, 5, 116, 35, 22, 1265, 4, 807, 176],\n",
       " [141, 8, 344, 52, 5, 116, 35, 22, 1265, 4, 807, 176, 417],\n",
       " [41, 5],\n",
       " [41, 5, 2012],\n",
       " [41, 5, 2012, 35],\n",
       " [41, 5, 2012, 35, 5],\n",
       " [41, 5, 2012, 35, 5, 18],\n",
       " [41, 5, 2012, 35, 5, 18, 1020],\n",
       " [17, 6],\n",
       " [17, 6, 3634],\n",
       " [17, 6, 3634, 2597],\n",
       " [17, 6, 3634, 2597, 22],\n",
       " [17, 6, 3634, 2597, 22, 1],\n",
       " [17, 6, 3634, 2597, 22, 1, 266],\n",
       " [5, 388],\n",
       " [5, 388, 2],\n",
       " [5, 388, 2, 1259],\n",
       " [5, 388, 2, 1259, 3],\n",
       " [5, 388, 2, 1259, 3, 205],\n",
       " [5, 388, 2, 1259, 3, 205, 126],\n",
       " [5, 388, 2, 1259, 3, 205, 126, 157],\n",
       " [200, 68],\n",
       " [200, 68, 418],\n",
       " [200, 68, 418, 59],\n",
       " [200, 68, 418, 59, 122],\n",
       " [200, 68, 418, 59, 122, 1266],\n",
       " [200, 68, 418, 59, 122, 1266, 120],\n",
       " [55, 5],\n",
       " [55, 5, 104],\n",
       " [55, 5, 104, 654],\n",
       " [55, 5, 104, 654, 2],\n",
       " [55, 5, 104, 654, 2, 4],\n",
       " [55, 5, 104, 654, 2, 4, 3635],\n",
       " [55, 5, 104, 654, 2, 4, 3635, 1696],\n",
       " [55, 5, 104, 654, 2, 4, 3635, 1696, 5],\n",
       " [55, 5, 104, 654, 2, 4, 3635, 1696, 5, 66],\n",
       " [8, 355],\n",
       " [8, 355, 3],\n",
       " [8, 355, 3, 21],\n",
       " [8, 355, 3, 21, 17],\n",
       " [8, 355, 3, 21, 17, 2],\n",
       " [8, 355, 3, 21, 17, 2, 13],\n",
       " [8, 355, 3, 21, 17, 2, 13, 3],\n",
       " [8, 355, 3, 21, 17, 2, 13, 3, 61],\n",
       " [8, 355, 3, 21, 17, 2, 13, 3, 61, 2598],\n",
       " [176, 335],\n",
       " [176, 335, 97],\n",
       " [176, 335, 97, 6],\n",
       " [176, 335, 97, 6, 539],\n",
       " [176, 335, 97, 6, 539, 6],\n",
       " [176, 335, 97, 6, 539, 6, 539],\n",
       " [176, 335, 97, 6, 539, 6, 539, 149],\n",
       " [176, 335, 97, 6, 539, 6, 539, 149, 91],\n",
       " [176, 335, 97, 6, 539, 6, 539, 149, 91, 12],\n",
       " [176, 335, 97, 6, 539, 6, 539, 149, 91, 12, 434],\n",
       " [176, 335, 97, 6, 539, 6, 539, 149, 91, 12, 434, 7],\n",
       " [176, 335, 97, 6, 539, 6, 539, 149, 91, 12, 434, 7, 316],\n",
       " [150, 21],\n",
       " [150, 21, 2],\n",
       " [150, 21, 2, 275],\n",
       " [5, 121],\n",
       " [5, 121, 20],\n",
       " [5, 121, 20, 47],\n",
       " [5, 121, 20, 47, 2599],\n",
       " [5, 139],\n",
       " [5, 139, 46],\n",
       " [5, 139, 46, 1],\n",
       " [5, 139, 46, 1, 3636],\n",
       " [5, 139, 46, 1, 3636, 14],\n",
       " [5, 139, 46, 1, 3636, 14, 11],\n",
       " [5, 102],\n",
       " [5, 102, 156],\n",
       " [5, 102, 156, 67],\n",
       " [5, 102, 156, 67, 115],\n",
       " [60, 20],\n",
       " [60, 20, 17],\n",
       " [60, 20, 17, 656],\n",
       " [60, 20, 17, 656, 859],\n",
       " [5, 121],\n",
       " [5, 121, 4],\n",
       " [5, 121, 4, 3637],\n",
       " [5, 121, 4, 3637, 335],\n",
       " [5, 121, 4, 3637, 335, 56],\n",
       " [5, 121, 4, 3637, 335, 56, 177],\n",
       " [5, 121, 4, 3637, 335, 56, 177, 12],\n",
       " [5, 121, 4, 3637, 335, 56, 177, 12, 3638],\n",
       " [5, 121, 4, 3637, 335, 56, 177, 12, 3638, 17],\n",
       " [5, 121, 4, 3637, 335, 56, 177, 12, 3638, 17, 36],\n",
       " [5, 121, 4, 3637, 335, 56, 177, 12, 3638, 17, 36, 711],\n",
       " [5, 121, 4, 3637, 335, 56, 177, 12, 3638, 17, 36, 711, 19],\n",
       " [5, 121, 4, 3637, 335, 56, 177, 12, 3638, 17, 36, 711, 19, 934],\n",
       " [755, 5],\n",
       " [755, 5, 214],\n",
       " [755, 5, 214, 265],\n",
       " [755, 5, 214, 265, 5],\n",
       " [755, 5, 214, 265, 5, 50],\n",
       " [755, 5, 214, 265, 5, 50, 60],\n",
       " [755, 5, 214, 265, 5, 50, 60, 82],\n",
       " [755, 5, 214, 265, 5, 50, 60, 82, 657],\n",
       " [54, 114],\n",
       " [54, 114, 76],\n",
       " [54, 114, 76, 2],\n",
       " [54, 114, 76, 2, 237],\n",
       " [2600, 11],\n",
       " [2600, 11, 25],\n",
       " [2600, 11, 25, 19],\n",
       " [2600, 11, 25, 19, 1697],\n",
       " [2600, 11, 25, 19, 1697, 1],\n",
       " [2600, 11, 25, 19, 1697, 1, 286],\n",
       " [2600, 11, 25, 19, 1697, 1, 286, 14],\n",
       " [2600, 11, 25, 19, 1697, 1, 286, 14, 44],\n",
       " [1698, 6],\n",
       " [1698, 6, 4],\n",
       " [1698, 6, 4, 268],\n",
       " [1698, 6, 4, 268, 253],\n",
       " [2601, 49],\n",
       " [2601, 49, 139],\n",
       " [2601, 49, 139, 3639],\n",
       " [2601, 49, 139, 3639, 3],\n",
       " [2601, 49, 139, 3639, 3, 167],\n",
       " [2601, 49, 139, 3639, 3, 167, 2007],\n",
       " [5, 34],\n",
       " [5, 34, 96],\n",
       " [5, 34, 96, 3],\n",
       " [5, 34, 96, 3, 435],\n",
       " [5, 34, 96, 3, 435, 24],\n",
       " [5, 34, 96, 3, 435, 24, 756],\n",
       " [5, 34, 96, 3, 435, 24, 756, 227],\n",
       " [5, 34, 96, 3, 435, 24, 756, 227, 486],\n",
       " [5, 34, 96, 3, 435, 24, 756, 227, 486, 72],\n",
       " [5, 34, 96, 3, 435, 24, 756, 227, 486, 72, 7],\n",
       " [5, 34, 96, 3, 435, 24, 756, 227, 486, 72, 7, 171],\n",
       " [5, 34, 96, 3, 435, 24, 756, 227, 486, 72, 7, 171, 12],\n",
       " [5, 34, 96, 3, 435, 24, 756, 227, 486, 72, 7, 171, 12, 4],\n",
       " [5, 34, 96, 3, 435, 24, 756, 227, 486, 72, 7, 171, 12, 4, 1138],\n",
       " [5, 34, 96, 3, 435, 24, 756, 227, 486, 72, 7, 171, 12, 4, 1138, 2602],\n",
       " [5, 92],\n",
       " [5, 92, 5],\n",
       " [5, 92, 5, 13],\n",
       " [5, 92, 5, 13, 4],\n",
       " [5, 92, 5, 13, 4, 808],\n",
       " [5, 92, 5, 13, 4, 808, 59],\n",
       " [5, 92, 5, 13, 4, 808, 59, 17],\n",
       " [17, 6],\n",
       " [17, 6, 3640],\n",
       " [2, 12],\n",
       " [2, 12, 608],\n",
       " [2, 12, 608, 2],\n",
       " [2, 12, 608, 2, 2013],\n",
       " [2, 12, 608, 2, 2013, 2014],\n",
       " [2, 12, 608, 2, 2013, 2014, 10],\n",
       " [2, 12, 608, 2, 2013, 2014, 10, 65],\n",
       " [2, 12, 608, 2, 2013, 2014, 10, 65, 526],\n",
       " [5, 21],\n",
       " [5, 21, 20],\n",
       " [5, 21, 20, 13],\n",
       " [5, 21, 20, 13, 47],\n",
       " [5, 21, 20, 13, 47, 459],\n",
       " [5, 21, 20, 13, 47, 459, 8],\n",
       " [5, 21, 20, 13, 47, 459, 8, 177],\n",
       " [5, 21, 20, 13, 47, 459, 8, 177, 3641],\n",
       " [55, 655],\n",
       " [55, 655, 18],\n",
       " [55, 655, 18, 3],\n",
       " [55, 655, 18, 3, 275],\n",
       " [55, 655, 18, 3, 275, 26],\n",
       " [55, 655, 18, 3, 275, 26, 1],\n",
       " [55, 655, 18, 3, 275, 26, 1, 563],\n",
       " [55, 655, 18, 3, 275, 26, 1, 563, 7],\n",
       " [55, 655, 18, 3, 275, 26, 1, 563, 7, 1],\n",
       " [55, 655, 18, 3, 275, 26, 1, 563, 7, 1, 401],\n",
       " [55, 655, 18, 3, 275, 26, 1, 563, 7, 1, 401, 6],\n",
       " [55, 655, 18, 3, 275, 26, 1, 563, 7, 1, 401, 6, 5],\n",
       " [55, 655, 18, 3, 275, 26, 1, 563, 7, 1, 401, 6, 5, 255],\n",
       " [55, 655, 18, 3, 275, 26, 1, 563, 7, 1, 401, 6, 5, 255, 34],\n",
       " [55, 655, 18, 3, 275, 26, 1, 563, 7, 1, 401, 6, 5, 255, 34, 81],\n",
       " [5, 166],\n",
       " [5, 166, 81],\n",
       " [5, 166, 81, 143],\n",
       " [5, 166, 81, 143, 11],\n",
       " [5, 166, 81, 143, 11, 151],\n",
       " [5, 166, 81, 143, 11, 151, 37],\n",
       " [5, 92],\n",
       " [5, 92, 24],\n",
       " [5, 92, 24, 436],\n",
       " [5, 92, 24, 436, 23],\n",
       " [5, 92, 24, 436, 23, 2],\n",
       " [5, 92, 24, 436, 23, 2, 51],\n",
       " [5, 92, 24, 436, 23, 2, 51, 1443],\n",
       " [5, 92, 24, 436, 23, 2, 51, 1443, 10],\n",
       " [5, 92, 24, 436, 23, 2, 51, 1443, 10, 95],\n",
       " [5, 92, 24, 436, 23, 2, 51, 1443, 10, 95, 7],\n",
       " [5, 92, 24, 436, 23, 2, 51, 1443, 10, 95, 7, 436],\n",
       " [45, 6],\n",
       " [45, 6, 20],\n",
       " [45, 6, 20, 303],\n",
       " [5, 166],\n",
       " [5, 166, 50],\n",
       " [5, 166, 50, 11],\n",
       " [935, 1267],\n",
       " [935, 1267, 54],\n",
       " [935, 1267, 54, 540],\n",
       " [935, 1267, 54, 540, 564],\n",
       " [119, 128],\n",
       " [119, 128, 712],\n",
       " [119, 128, 712, 162],\n",
       " [119, 128, 712, 162, 37],\n",
       " [119, 128, 712, 162, 37, 3642],\n",
       " [119, 128, 712, 162, 37, 3642, 419],\n",
       " [55, 2],\n",
       " [55, 2, 103],\n",
       " [55, 2, 103, 22],\n",
       " [55, 2, 103, 22, 1],\n",
       " [55, 2, 103, 22, 1, 3643],\n",
       " [55, 2, 103, 22, 1, 3643, 91],\n",
       " [55, 2, 103, 22, 1, 3643, 91, 34],\n",
       " [55, 2, 103, 22, 1, 3643, 91, 34, 255],\n",
       " [55, 2, 103, 22, 1, 3643, 91, 34, 255, 416],\n",
       " [55, 2, 103, 22, 1, 3643, 91, 34, 255, 416, 98],\n",
       " [267, 4],\n",
       " [267, 4, 210],\n",
       " [267, 4, 210, 31],\n",
       " [267, 4, 210, 31, 17],\n",
       " [267, 4, 210, 31, 17, 2],\n",
       " [267, 4, 210, 31, 17, 2, 34],\n",
       " [267, 4, 210, 31, 17, 2, 34, 93],\n",
       " [2, 36],\n",
       " [2, 36, 21],\n",
       " [2, 36, 21, 11],\n",
       " [24, 2603],\n",
       " [24, 2603, 233],\n",
       " [24, 2603, 233, 315],\n",
       " [24, 2603, 233, 315, 354],\n",
       " [24, 2603, 233, 315, 354, 55],\n",
       " [24, 2603, 233, 315, 354, 55, 5],\n",
       " [24, 2603, 233, 315, 354, 55, 5, 2604],\n",
       " [24, 2603, 233, 315, 354, 55, 5, 2604, 1699],\n",
       " [5, 345],\n",
       " [5, 345, 5],\n",
       " [5, 345, 5, 104],\n",
       " [5, 345, 5, 104, 39],\n",
       " [5, 345, 5, 104, 39, 3],\n",
       " [5, 345, 5, 104, 39, 3, 336],\n",
       " [5, 1268],\n",
       " [5, 1268, 11],\n",
       " [5, 1268, 11, 41],\n",
       " [5, 1268, 11, 41, 31],\n",
       " [5, 1268, 11, 41, 31, 12],\n",
       " [5, 1268, 11, 41, 31, 12, 4],\n",
       " [5, 1268, 11, 41, 31, 12, 4, 163],\n",
       " [5, 1268, 11, 41, 31, 12, 4, 163, 7],\n",
       " [5, 1268, 11, 41, 31, 12, 4, 163, 7, 68],\n",
       " [5, 13],\n",
       " [5, 13, 3],\n",
       " [5, 13, 3, 39],\n",
       " [5, 13, 3, 39, 3],\n",
       " [5, 13, 3, 39, 3, 402],\n",
       " [136, 17],\n",
       " [136, 17, 5],\n",
       " [136, 17, 5, 178],\n",
       " [136, 17, 5, 178, 56],\n",
       " [136, 17, 5, 178, 56, 484],\n",
       " [136, 17, 5, 178, 56, 484, 5],\n",
       " [136, 17, 5, 178, 56, 484, 5, 2015],\n",
       " [136, 17, 5, 178, 56, 484, 5, 2015, 17],\n",
       " [136, 17, 5, 178, 56, 484, 5, 2015, 17, 5],\n",
       " [136, 17, 5, 178, 56, 484, 5, 2015, 17, 5, 2605],\n",
       " [136, 17, 5, 178, 56, 484, 5, 2015, 17, 5, 2605, 24],\n",
       " [136, 17, 5, 178, 56, 484, 5, 2015, 17, 5, 2605, 24, 3644],\n",
       " [136, 17, 5, 178, 56, 484, 5, 2015, 17, 5, 2605, 24, 3644, 22],\n",
       " [136, 17, 5, 178, 56, 484, 5, 2015, 17, 5, 2605, 24, 3644, 22, 122],\n",
       " [136, 17, 5, 178, 56, 484, 5, 2015, 17, 5, 2605, 24, 3644, 22, 122, 86],\n",
       " [5, 279],\n",
       " [5, 279, 275],\n",
       " [5, 279, 275, 2],\n",
       " [5, 279, 275, 2, 179],\n",
       " [5, 279, 275, 2, 179, 603],\n",
       " [5, 279, 275, 2, 179, 603, 302],\n",
       " [11, 75],\n",
       " [11, 75, 3645],\n",
       " [11, 75, 3645, 276],\n",
       " [11, 75, 3645, 276, 346],\n",
       " [227, 486],\n",
       " [227, 486, 29],\n",
       " [227, 486, 29, 3646],\n",
       " [227, 486, 29, 3646, 57],\n",
       " [227, 486, 29, 3646, 57, 215],\n",
       " [227, 486, 29, 3646, 57, 215, 3647],\n",
       " [1, 713],\n",
       " [1, 713, 25],\n",
       " [1, 713, 25, 58],\n",
       " [1, 713, 25, 58, 2],\n",
       " [1, 713, 25, 58, 2, 3],\n",
       " [1, 713, 25, 58, 2, 3, 214],\n",
       " [1, 713, 25, 58, 2, 3, 214, 1],\n",
       " [1, 713, 25, 58, 2, 3, 214, 1, 3648],\n",
       " [565, 14],\n",
       " [565, 14, 256],\n",
       " [565, 14, 256, 2597],\n",
       " [565, 14, 256, 2597, 3],\n",
       " [565, 14, 256, 2597, 3, 30],\n",
       " [565, 14, 256, 2597, 3, 30, 22],\n",
       " [565, 14, 256, 2597, 3, 30, 22, 123],\n",
       " [565, 14, 256, 2597, 3, 30, 22, 123, 150],\n",
       " [565, 14, 256, 2597, 3, 30, 22, 123, 150, 68],\n",
       " [565, 14, 256, 2597, 3, 30, 22, 123, 150, 68, 61],\n",
       " [565, 14, 256, 2597, 3, 30, 22, 123, 150, 68, 61, 30],\n",
       " [565, 14, 256, 2597, 3, 30, 22, 123, 150, 68, 61, 30, 14],\n",
       " [565, 14, 256, 2597, 3, 30, 22, 123, 150, 68, 61, 30, 14, 47],\n",
       " [565, 14, 256, 2597, 3, 30, 22, 123, 150, 68, 61, 30, 14, 47, 3649],\n",
       " [17, 757],\n",
       " [17, 757, 24],\n",
       " [17, 757, 24, 1700],\n",
       " [565, 14],\n",
       " [565, 14, 10],\n",
       " [565, 14, 10, 1445],\n",
       " [2606, 57],\n",
       " [2606, 57, 389],\n",
       " [2606, 57, 389, 1701],\n",
       " [55, 2],\n",
       " [55, 2, 166],\n",
       " [55, 2, 166, 81],\n",
       " [55, 2, 166, 81, 30],\n",
       " [55, 2, 166, 81, 30, 17],\n",
       " [55, 2, 166, 81, 30, 17, 95],\n",
       " [55, 2, 166, 81, 30, 17, 95, 484],\n",
       " [55, 2, 166, 81, 30, 17, 95, 484, 2],\n",
       " [55, 2, 166, 81, 30, 17, 95, 484, 2, 1702],\n",
       " [55, 2, 166, 81, 30, 17, 95, 484, 2, 1702, 166],\n",
       " [55, 2, 166, 81, 30, 17, 95, 484, 2, 1702, 166, 81],\n",
       " [55, 2, 166, 81, 30, 17, 95, 484, 2, 1702, 166, 81, 30],\n",
       " [11, 420],\n",
       " [11, 420, 2607],\n",
       " [11, 420, 2607, 30],\n",
       " [11, 420, 2607, 30, 20],\n",
       " [11, 420, 2607, 30, 20, 3],\n",
       " [11, 420, 2607, 30, 20, 3, 93],\n",
       " [11, 420, 2607, 30, 20, 3, 93, 2],\n",
       " [11, 420, 2607, 30, 20, 3, 93, 2, 2608],\n",
       " [11, 420, 2607, 30, 20, 3, 93, 2, 2608, 14],\n",
       " [11, 420, 2607, 30, 20, 3, 93, 2, 2608, 14, 4],\n",
       " [11, 420, 2607, 30, 20, 3, 93, 2, 2608, 14, 4, 304],\n",
       " [11, 420, 2607, 30, 20, 3, 93, 2, 2608, 14, 4, 304, 129],\n",
       " [5, 34],\n",
       " [5, 34, 81],\n",
       " [5, 34, 81, 26],\n",
       " [5, 34, 81, 26, 2],\n",
       " [5, 34, 81, 26, 2, 416],\n",
       " [24, 809],\n",
       " [24, 809, 51],\n",
       " [24, 809, 51, 116],\n",
       " [24, 809, 51, 116, 3],\n",
       " [24, 809, 51, 116, 3, 19],\n",
       " [24, 809, 51, 116, 3, 19, 1139],\n",
       " [24, 809, 51, 116, 3, 19, 1139, 14],\n",
       " [24, 809, 51, 116, 3, 19, 1139, 14, 265],\n",
       " [2, 604],\n",
       " [2, 604, 3],\n",
       " [2, 604, 3, 156],\n",
       " [2, 604, 3, 156, 30],\n",
       " [2, 604, 3, 156, 30, 59],\n",
       " [2, 604, 3, 156, 30, 59, 1021],\n",
       " [2609, 115],\n",
       " [2609, 115, 60],\n",
       " [2609, 115, 60, 255],\n",
       " [2609, 115, 60, 255, 2610],\n",
       " [55, 5],\n",
       " [55, 5, 604],\n",
       " [55, 5, 604, 3],\n",
       " [55, 5, 604, 3, 2611],\n",
       " [55, 5, 604, 3, 2611, 2],\n",
       " [55, 5, 604, 3, 2611, 2, 5],\n",
       " [55, 5, 604, 3, 2611, 2, 5, 66],\n",
       " [55, 5, 604, 3, 2611, 2, 5, 66, 156],\n",
       " [55, 5, 604, 3, 2611, 2, 5, 66, 156, 2],\n",
       " [55, 5, 604, 3, 2611, 2, 5, 66, 156, 2, 26],\n",
       " [55, 5, 604, 3, 2611, 2, 5, 66, 156, 2, 26, 5],\n",
       " [55, 5, 604, 3, 2611, 2, 5, 66, 156, 2, 26, 5, 2612],\n",
       " [55, 5, 604, 3, 2611, 2, 5, 66, 156, 2, 26, 5, 2612, 59],\n",
       " [55, 5, 604, 3, 2611, 2, 5, 66, 156, 2, 26, 5, 2612, 59, 4],\n",
       " [55, 5, 604, 3, 2611, 2, 5, 66, 156, 2, 26, 5, 2612, 59, 4, 234],\n",
       " [55, 5, 604, 3, 2611, 2, 5, 66, 156, 2, 26, 5, 2612, 59, 4, 234, 1703],\n",
       " [55, 5, 604, 3, 2611, 2, 5, 66, 156, 2, 26, 5, 2612, 59, 4, 234, 1703, 658],\n",
       " [69, 102],\n",
       " [69, 102, 421],\n",
       " [69, 102, 421, 211],\n",
       " [69, 102, 421, 211, 37],\n",
       " [69, 102, 421, 211, 37, 758],\n",
       " [31, 12],\n",
       " [31, 12, 79],\n",
       " [31, 12, 79, 374],\n",
       " [31, 12, 79, 374, 17],\n",
       " [31, 12, 79, 374, 17, 5],\n",
       " [31, 12, 79, 374, 17, 5, 34],\n",
       " [31, 12, 79, 374, 17, 5, 34, 356],\n",
       " [5, 34],\n",
       " [5, 34, 50],\n",
       " [5, 34, 50, 11],\n",
       " [5, 34, 50, 11, 41],\n",
       " [5, 34, 50, 11, 41, 2016],\n",
       " [5, 34, 50, 11, 41, 2016, 97],\n",
       " [5, 34, 50, 11, 41, 2016, 97, 81],\n",
       " [5, 34, 50, 11, 41, 2016, 97, 81, 98],\n",
       " [5, 34, 50, 11, 41, 2016, 97, 81, 98, 70],\n",
       " [5, 34, 50, 11, 41, 2016, 97, 81, 98, 70, 83],\n",
       " [5, 34, 50, 11, 41, 2016, 97, 81, 98, 70, 83, 5],\n",
       " [5, 34, 50, 11, 41, 2016, 97, 81, 98, 70, 83, 5, 21],\n",
       " [5, 34, 50, 11, 41, 2016, 97, 81, 98, 70, 83, 5, 21, 102],\n",
       " [5, 34, 50, 11, 41, 2016, 97, 81, 98, 70, 83, 5, 21, 102, 460],\n",
       " [5, 34, 50, 11, 41, 2016, 97, 81, 98, 70, 83, 5, 21, 102, 460, 860],\n",
       " [5, 34, 50, 11, 41, 2016, 97, 81, 98, 70, 83, 5, 21, 102, 460, 860, 3650],\n",
       " [119, 255],\n",
       " [119, 255, 20],\n",
       " [119, 255, 20, 1022],\n",
       " [5, 131],\n",
       " [5, 131, 3],\n",
       " [5, 131, 3, 275],\n",
       " [5, 131, 3, 275, 2],\n",
       " [5, 131, 3, 275, 2, 4],\n",
       " [5, 131, 3, 275, 2, 4, 1704],\n",
       " [5, 131, 3, 275, 2, 4, 1704, 422],\n",
       " [5, 34],\n",
       " [5, 34, 81],\n",
       " [5, 34, 81, 54],\n",
       " [5, 34, 81, 54, 3],\n",
       " [5, 34, 81, 54, 3, 3651],\n",
       " [5, 34, 81, 54, 3, 3651, 11],\n",
       " [5, 34, 81, 54, 3, 3651, 11, 437],\n",
       " [5, 34, 81, 54, 3, 3651, 11, 437, 60],\n",
       " [5, 34, 81, 54, 3, 3651, 11, 437, 60, 82],\n",
       " [5, 34, 81, 54, 3, 3651, 11, 437, 60, 82, 1705],\n",
       " [5, 1019],\n",
       " [5, 1019, 13],\n",
       " [5, 1019, 13, 388],\n",
       " [5, 1019, 13, 388, 5],\n",
       " [5, 1019, 13, 388, 5, 66],\n",
       " [5, 1019, 13, 388, 5, 66, 861],\n",
       " [5, 1019, 13, 388, 5, 66, 861, 103],\n",
       " [5, 1019, 13, 388, 5, 66, 861, 103, 35],\n",
       " [5, 1019, 13, 388, 5, 66, 861, 103, 35, 3652],\n",
       " [5, 1019, 13, 388, 5, 66, 861, 103, 35, 3652, 8],\n",
       " [5, 1019, 13, 388, 5, 66, 861, 103, 35, 3652, 8, 3653],\n",
       " [36, 11],\n",
       " [36, 11, 19],\n",
       " [36, 11, 19, 3654],\n",
       " [36, 11, 19, 3654, 8],\n",
       " [36, 11, 19, 3654, 8, 415],\n",
       " [36, 11, 19, 3654, 8, 415, 95],\n",
       " [53, 69],\n",
       " [53, 69, 25],\n",
       " [53, 69, 25, 81],\n",
       " [5, 305],\n",
       " [5, 305, 4],\n",
       " [5, 305, 4, 1023],\n",
       " [5, 305, 4, 1023, 56],\n",
       " [5, 305, 4, 1023, 56, 5],\n",
       " [5, 305, 4, 1023, 56, 5, 305],\n",
       " [5, 305, 4, 1023, 56, 5, 305, 11],\n",
       " [5, 305, 4, 1023, 56, 5, 305, 11, 71],\n",
       " [5, 305, 4, 1023, 56, 5, 305, 11, 71, 566],\n",
       " [5, 305, 4, 1023, 56, 5, 305, 11, 71, 566, 17],\n",
       " [5, 305, 4, 1023, 56, 5, 305, 11, 71, 566, 17, 11],\n",
       " [5, 305, 4, 1023, 56, 5, 305, 11, 71, 566, 17, 11, 102],\n",
       " [5, 305, 4, 1023, 56, 5, 305, 11, 71, 566, 17, 11, 102, 19],\n",
       " [5, 305, 4, 1023, 56, 5, 305, 11, 71, 566, 17, 11, 102, 19, 1],\n",
       " [5, 305, 4, 1023, 56, 5, 305, 11, 71, 566, 17, 11, 102, 19, 1, 159],\n",
       " [5, 305, 4, 1023, 56, 5, 305, 11, 71, 566, 17, 11, 102, 19, 1, 159, 1023],\n",
       " [11, 390],\n",
       " [11, 390, 659],\n",
       " [11, 390, 659, 3],\n",
       " [11, 390, 659, 3, 30],\n",
       " [936, 17],\n",
       " [936, 17, 107],\n",
       " [936, 17, 107, 60],\n",
       " [936, 17, 107, 60, 20],\n",
       " [936, 17, 107, 60, 20, 71],\n",
       " [936, 17, 107, 60, 20, 71, 656],\n",
       " [5, 50],\n",
       " [5, 50, 3655],\n",
       " [26, 76],\n",
       " [26, 76, 2],\n",
       " [26, 76, 2, 347],\n",
       " [53, 457],\n",
       " [53, 457, 20],\n",
       " [53, 457, 20, 24],\n",
       " [53, 457, 20, 24, 126],\n",
       " [53, 457, 20, 24, 126, 3656],\n",
       " [60, 82],\n",
       " [60, 82, 287],\n",
       " [60, 82, 287, 17],\n",
       " [60, 82, 287, 17, 5],\n",
       " [60, 82, 287, 17, 5, 34],\n",
       " [60, 82, 287, 17, 5, 34, 131],\n",
       " [60, 82, 287, 17, 5, 34, 131, 3],\n",
       " [60, 82, 287, 17, 5, 34, 131, 3, 435],\n",
       " [60, 82, 287, 17, 5, 34, 131, 3, 435, 1024],\n",
       " [2, 139],\n",
       " [2, 139, 13],\n",
       " [2, 139, 13, 327],\n",
       " [2, 139, 13, 327, 77],\n",
       " [2, 139, 13, 327, 77, 26],\n",
       " [5, 25],\n",
       " [5, 25, 293],\n",
       " [5, 25, 293, 3657],\n",
       " [5, 25, 293, 3657, 484],\n",
       " [5, 25, 293, 3657, 484, 752],\n",
       " [5, 25, 293, 3657, 484, 752, 7],\n",
       " [5, 25, 293, 3657, 484, 752, 7, 2017],\n",
       " [5, 25, 293, 3657, 484, 752, 7, 2017, 3],\n",
       " [5, 25, 293, 3657, 484, 752, 7, 2017, 3, 3658],\n",
       " [5, 25, 293, 3657, 484, 752, 7, 2017, 3, 3658, 2],\n",
       " [143, 6],\n",
       " [143, 6, 1],\n",
       " [143, 6, 1, 210],\n",
       " [5, 36],\n",
       " [5, 36, 109],\n",
       " [5, 36, 109, 357],\n",
       " [60, 20],\n",
       " [60, 20, 98],\n",
       " [60, 20, 98, 7],\n",
       " [60, 20, 98, 7, 4],\n",
       " [60, 20, 98, 7, 4, 232],\n",
       " [60, 20, 98, 7, 4, 232, 6],\n",
       " [60, 20, 98, 7, 4, 232, 6, 11],\n",
       " [5, 337],\n",
       " [5, 337, 2],\n",
       " [5, 34],\n",
       " [5, 34, 50],\n",
       " [5, 34, 50, 2],\n",
       " [5, 34, 50, 2, 859],\n",
       " [5, 121],\n",
       " [5, 121, 1446],\n",
       " [5, 34],\n",
       " [5, 34, 96],\n",
       " [5, 34, 96, 3],\n",
       " [5, 34, 96, 3, 357],\n",
       " [5, 34, 96, 3, 357, 17],\n",
       " [5, 34, 96, 3, 357, 17, 114],\n",
       " [150, 34],\n",
       " [150, 34, 2],\n",
       " [150, 34, 2, 64],\n",
       " [150, 34, 2, 64, 610],\n",
       " [150, 34, 2, 64, 610, 78],\n",
       " [56, 1],\n",
       " [56, 1, 2018],\n",
       " [56, 1, 2018, 390],\n",
       " [56, 1, 2018, 390, 2613],\n",
       " [5, 487],\n",
       " [5, 487, 13],\n",
       " [5, 487, 13, 3660],\n",
       " [5, 487, 13, 3660, 108],\n",
       " [5, 34],\n",
       " [5, 34, 81],\n",
       " [5, 34, 81, 26],\n",
       " [5, 34, 81, 26, 3],\n",
       " [5, 34, 81, 26, 3, 21],\n",
       " [5, 34, 81, 26, 3, 21, 859],\n",
       " [11, 6],\n",
       " [11, 6, 2019],\n",
       " [11, 6, 2019, 17],\n",
       " [11, 6, 2019, 17, 5],\n",
       " [11, 6, 2019, 17, 5, 39],\n",
       " [11, 6, 2019, 17, 5, 39, 3],\n",
       " [11, 6, 2019, 17, 5, 39, 3, 1269],\n",
       " [11, 6, 2019, 17, 5, 39, 3, 1269, 861],\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences\n",
    "max_seq_length = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_length, padding='pre')\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "y = keras.utils.to_categorical(y, num_classes=vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sbhar\\anaconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Define LSTM Model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, 128, input_length=max_seq_length - 1),\n",
    "    keras.layers.LSTM(256, return_sequences=True),\n",
    "    keras.layers.LSTM(256),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2183/2183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1499s\u001b[0m 687ms/step - accuracy: 0.0519 - loss: 6.7083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24103228800>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Train Model\n",
    "model.fit(X, y, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate Sentence Completions\n",
    "def generate_text(seed_text, next_words=10):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_length - 1, padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where is the meeting\n"
     ]
    }
   ],
   "source": [
    "# Example Completion\n",
    "print(generate_text(\"where is  \", 2 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Artificial intelligence is transforming  the meeting of\n"
     ]
    }
   ],
   "source": [
    "# Example Completion\n",
    "print(generate_text(\"Artificial intelligence is transforming \", 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "# Load WikiText-2 dataset\n",
    "from torchtext.datasets import WikiText2\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Tokenize the dataset\n",
    "counter = Counter()\n",
    "for line in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = Vocab(counter, min_freq=2, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "print(f\"Vocabulary Size: {len(vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tensor(text_iter, vocab, tokenizer, seq_length=30):\n",
    "    sequences = []\n",
    "    for line in text_iter:\n",
    "        tokens = tokenizer(line)\n",
    "        indices = [vocab[token] for token in tokens]\n",
    "        sequences.extend(indices)\n",
    "    \n",
    "    # Convert list to PyTorch tensor\n",
    "    text_tensor = torch.tensor(sequences, dtype=torch.long)\n",
    "    \n",
    "    # Create input-output pairs\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(len(text_tensor) - seq_length):\n",
    "        inputs.append(text_tensor[i:i+seq_length])\n",
    "        targets.append(text_tensor[i+1:i+seq_length+1])\n",
    "\n",
    "    return torch.stack(inputs), torch.stack(targets)\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "X_train, y_train = text_to_tensor(train_iter, vocab, tokenizer)\n",
    "print(f\"Training Data Shape: {X_train.shape}, {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "# Define model parameters\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "\n",
    "# Instantiate model\n",
    "model = LSTMLanguageModel(vocab_size, embed_size, hidden_size, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "seq_length = 30\n",
    "hidden = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        inputs = X_train[i:i+batch_size].to(device)\n",
    "        targets = y_train[i:i+batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(inputs, hidden)\n",
    "\n",
    "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(X_train):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Example Usage\u001b[39;00m\n\u001b[0;32m     24\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe future of AI is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 25\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m generate_text(prompt, model, \u001b[43mvocab\u001b[49m, tokenizer)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated_text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_text(prompt, model, vocab, tokenizer, seq_length=10, max_words=50):\n",
    "    model.eval()\n",
    "    \n",
    "    words = tokenizer(prompt)\n",
    "    indices = [vocab[token] for token in words]\n",
    "    input_seq = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    generated_words = words\n",
    "\n",
    "    hidden = None\n",
    "    for _ in range(max_words):\n",
    "        output, hidden = model(input_seq, hidden)\n",
    "        next_word_index = output.argmax(dim=-1)[:, -1].item()\n",
    "        next_word = vocab.lookup_token(next_word_index)\n",
    "        \n",
    "        generated_words.append(next_word)\n",
    "        input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[next_word_index]], dtype=torch.long).to(device)], dim=1)\n",
    "\n",
    "    return \" \".join(generated_words)\n",
    "\n",
    "# Example Usage\n",
    "prompt = \"The future of AI is\"\n",
    "generated_text = generate_text(prompt, model, vocab, tokenizer)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# language translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file_path = r\"Dataset\\\\en.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    sentences = file.readlines()\n",
    "english_sentences = [sentence.strip() for sentence in sentences]\n",
    "\n",
    "\n",
    "file_path = r\"Dataset\\\\ta.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    sentences = file.readlines()\n",
    "tamil_sentences = [sentence.strip() for sentence in sentences]\n",
    "\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(sentences):\n",
    "    tokenized = [sentence.lower().replace(\",\", \"\").replace(\"?\", \"\").split() for sentence in sentences]\n",
    "    return tokenized\n",
    "\n",
    "# Create vocabularies\n",
    "def build_vocab(tokenized_sentences):\n",
    "    counter = Counter()\n",
    "    for sentence in tokenized_sentences:\n",
    "        counter.update(sentence)\n",
    "    vocab = {word: i + 2 for i, (word, _) in enumerate(counter.most_common())}  # Start index from 2\n",
    "    vocab[\"<pad>\"] = 0\n",
    "    vocab[\"<unk>\"] = 1\n",
    "    return vocab\n",
    "\n",
    "# Convert sentences to numerical sequences\n",
    "def encode_sentences(tokenized_sentences, vocab):\n",
    "    return [[vocab.get(word, vocab[\"<unk>\"]) for word in sentence] for sentence in tokenized_sentences]\n",
    "\n",
    "# Tokenization\n",
    "eng_tokenized = tokenize(english_sentences)\n",
    "tam_tokenized = tokenize(tamil_sentences)\n",
    "\n",
    "# Build vocabulary\n",
    "eng_vocab = build_vocab(eng_tokenized)\n",
    "tam_vocab = build_vocab(tam_tokenized)\n",
    "\n",
    "# Convert text to sequences\n",
    "eng_sequences = encode_sentences(eng_tokenized, eng_vocab)\n",
    "tam_sequences = encode_sentences(tam_tokenized, tam_vocab)\n",
    "\n",
    "# Pad sequences\n",
    "eng_sequences = pad_sequence([torch.tensor(seq) for seq in eng_sequences], batch_first=True, padding_value=0)\n",
    "tam_sequences = pad_sequence([torch.tensor(seq) for seq in tam_sequences], batch_first=True, padding_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the LSTM Encoder-Decoder Model\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embed_dim=128, hidden_dim=256):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_embedding = nn.Embedding(input_dim, embed_dim, padding_idx=0)\n",
    "        self.encoder_lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_embedding = nn.Embedding(output_dim, embed_dim, padding_idx=0)\n",
    "        self.decoder_lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        # Encoder\n",
    "        enc_emb = self.encoder_embedding(encoder_input)\n",
    "        _, (hidden, cell) = self.encoder_lstm(enc_emb)\n",
    "\n",
    "        # Decoder\n",
    "        dec_emb = self.decoder_embedding(decoder_input)\n",
    "        decoder_output, _ = self.decoder_lstm(dec_emb, (hidden, cell))\n",
    "        output = self.fc(decoder_output)  # Predict next words\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model setup\n",
    "input_dim = len(eng_vocab)\n",
    "output_dim = len(tam_vocab)\n",
    "model = Seq2SeqModel(input_dim, output_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Prepare training data\n",
    "X_train = eng_sequences\n",
    "y_train = tam_sequences\n",
    "\n",
    "# Shift decoder input (teacher forcing)\n",
    "decoder_input = y_train[:, :-1]  # Remove last token\n",
    "decoder_target = y_train[:, 1:]  # Remove first token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1000], Loss: 5.9562\n",
      "Epoch [10/1000], Loss: 5.4402\n",
      "Epoch [20/1000], Loss: 4.1186\n",
      "Epoch [30/1000], Loss: 2.4821\n",
      "Epoch [40/1000], Loss: 1.1143\n",
      "Epoch [50/1000], Loss: 0.4310\n",
      "Epoch [60/1000], Loss: 0.1852\n",
      "Epoch [70/1000], Loss: 0.0945\n",
      "Epoch [80/1000], Loss: 0.0576\n",
      "Epoch [90/1000], Loss: 0.0402\n",
      "Epoch [100/1000], Loss: 0.0308\n",
      "Epoch [110/1000], Loss: 0.0251\n",
      "Epoch [120/1000], Loss: 0.0212\n",
      "Epoch [130/1000], Loss: 0.0183\n",
      "Epoch [140/1000], Loss: 0.0161\n",
      "Epoch [150/1000], Loss: 0.0143\n",
      "Epoch [160/1000], Loss: 0.0129\n",
      "Epoch [170/1000], Loss: 0.0116\n",
      "Epoch [180/1000], Loss: 0.0106\n",
      "Epoch [190/1000], Loss: 0.0097\n",
      "Epoch [200/1000], Loss: 0.0089\n",
      "Epoch [210/1000], Loss: 0.0082\n",
      "Epoch [220/1000], Loss: 0.0076\n",
      "Epoch [230/1000], Loss: 0.0071\n",
      "Epoch [240/1000], Loss: 0.0066\n",
      "Epoch [250/1000], Loss: 0.0062\n",
      "Epoch [260/1000], Loss: 0.0058\n",
      "Epoch [270/1000], Loss: 0.0055\n",
      "Epoch [280/1000], Loss: 0.0052\n",
      "Epoch [290/1000], Loss: 0.0049\n",
      "Epoch [300/1000], Loss: 0.0046\n",
      "Epoch [310/1000], Loss: 0.0044\n",
      "Epoch [320/1000], Loss: 0.0042\n",
      "Epoch [330/1000], Loss: 0.0040\n",
      "Epoch [340/1000], Loss: 0.0038\n",
      "Epoch [350/1000], Loss: 0.0036\n",
      "Epoch [360/1000], Loss: 0.0034\n",
      "Epoch [370/1000], Loss: 0.0033\n",
      "Epoch [380/1000], Loss: 0.0031\n",
      "Epoch [390/1000], Loss: 0.0030\n",
      "Epoch [400/1000], Loss: 0.0029\n",
      "Epoch [410/1000], Loss: 0.0028\n",
      "Epoch [420/1000], Loss: 0.0027\n",
      "Epoch [430/1000], Loss: 0.0026\n",
      "Epoch [440/1000], Loss: 0.0025\n",
      "Epoch [450/1000], Loss: 0.0024\n",
      "Epoch [460/1000], Loss: 0.0023\n",
      "Epoch [470/1000], Loss: 0.0022\n",
      "Epoch [480/1000], Loss: 0.0021\n",
      "Epoch [490/1000], Loss: 0.0021\n",
      "Epoch [500/1000], Loss: 0.0020\n",
      "Epoch [510/1000], Loss: 0.0019\n",
      "Epoch [520/1000], Loss: 0.0019\n",
      "Epoch [530/1000], Loss: 0.0018\n",
      "Epoch [540/1000], Loss: 0.0018\n",
      "Epoch [550/1000], Loss: 0.0017\n",
      "Epoch [560/1000], Loss: 0.0017\n",
      "Epoch [570/1000], Loss: 0.0016\n",
      "Epoch [580/1000], Loss: 0.0016\n",
      "Epoch [590/1000], Loss: 0.0015\n",
      "Epoch [600/1000], Loss: 0.0015\n",
      "Epoch [610/1000], Loss: 0.0014\n",
      "Epoch [620/1000], Loss: 0.0014\n",
      "Epoch [630/1000], Loss: 0.0014\n",
      "Epoch [640/1000], Loss: 0.0013\n",
      "Epoch [650/1000], Loss: 0.0013\n",
      "Epoch [660/1000], Loss: 0.0013\n",
      "Epoch [670/1000], Loss: 0.0012\n",
      "Epoch [680/1000], Loss: 0.0012\n",
      "Epoch [690/1000], Loss: 0.0012\n",
      "Epoch [700/1000], Loss: 0.0011\n",
      "Epoch [710/1000], Loss: 0.0011\n",
      "Epoch [720/1000], Loss: 0.0011\n",
      "Epoch [730/1000], Loss: 0.0011\n",
      "Epoch [740/1000], Loss: 0.0010\n",
      "Epoch [750/1000], Loss: 0.0010\n",
      "Epoch [760/1000], Loss: 0.0010\n",
      "Epoch [770/1000], Loss: 0.0010\n",
      "Epoch [780/1000], Loss: 0.0009\n",
      "Epoch [790/1000], Loss: 0.0009\n",
      "Epoch [800/1000], Loss: 0.0009\n",
      "Epoch [810/1000], Loss: 0.0009\n",
      "Epoch [820/1000], Loss: 0.0009\n",
      "Epoch [830/1000], Loss: 0.0008\n",
      "Epoch [840/1000], Loss: 0.0008\n",
      "Epoch [850/1000], Loss: 0.0008\n",
      "Epoch [860/1000], Loss: 0.0008\n",
      "Epoch [870/1000], Loss: 0.0008\n",
      "Epoch [880/1000], Loss: 0.0008\n",
      "Epoch [890/1000], Loss: 0.0008\n",
      "Epoch [900/1000], Loss: 0.0007\n",
      "Epoch [910/1000], Loss: 0.0007\n",
      "Epoch [920/1000], Loss: 0.0007\n",
      "Epoch [930/1000], Loss: 0.0007\n",
      "Epoch [940/1000], Loss: 0.0007\n",
      "Epoch [950/1000], Loss: 0.0007\n",
      "Epoch [960/1000], Loss: 0.0007\n",
      "Epoch [970/1000], Loss: 0.0006\n",
      "Epoch [980/1000], Loss: 0.0006\n",
      "Epoch [990/1000], Loss: 0.0006\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "batch_size = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_train, decoder_input)\n",
    "    loss = criterion(outputs.view(-1, output_dim), decoder_target.reshape(-1))\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Translation function\n",
    "def translate(sentence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequence = encode_sentences(tokenize([sentence]), eng_vocab)\n",
    "        sequence = pad_sequence([torch.tensor(seq) for seq in sequence], batch_first=True, padding_value=0)\n",
    "        \n",
    "        decoder_input = torch.zeros((1, sequence.shape[1]), dtype=torch.long)  # Empty decoder input\n",
    "        states_value = model(sequence, decoder_input)\n",
    "        predicted_seq = torch.argmax(states_value, dim=-1).squeeze(0).tolist()\n",
    "        \n",
    "        output_sentence = ' '.join([word for word, index in tam_vocab.items() if index in predicted_seq])\n",
    "        return output_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "நான்\n"
     ]
    }
   ],
   "source": [
    "# Test Translation\n",
    "print(translate(\"I like that movie.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input Shape: (4, 4)\n",
      "Decoder Input Shape: (4, 3)\n",
      "Decoder Target Shape: (4, 3)\n",
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.0625 - loss: 2.6386\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.4375 - loss: 2.6208\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6250 - loss: 2.6025\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6250 - loss: 2.5831\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.6250 - loss: 2.5620\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6250 - loss: 2.5385\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.6250 - loss: 2.5119\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6250 - loss: 2.4813\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.5625 - loss: 2.4458\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6250 - loss: 2.4040\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6250 - loss: 2.3548\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6250 - loss: 2.2963\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5625 - loss: 2.2267\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5625 - loss: 2.1441\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5000 - loss: 2.0466\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5000 - loss: 1.9337\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 1.8080\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5000 - loss: 1.6775\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 1.5544\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.5000 - loss: 1.4442\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.5000 - loss: 1.3378\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5000 - loss: 1.2256\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5000 - loss: 1.1110\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5625 - loss: 1.0082\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.6250 - loss: 0.9294\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.6875 - loss: 0.8630\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7500 - loss: 0.7938\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7500 - loss: 0.7188\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8125 - loss: 0.6448\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8125 - loss: 0.5807\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8125 - loss: 0.5309\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8750 - loss: 0.4865\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8750 - loss: 0.4375\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8750 - loss: 0.3867\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8750 - loss: 0.3437\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8750 - loss: 0.3101\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8750 - loss: 0.2757\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8750 - loss: 0.2385\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8750 - loss: 0.2067\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8750 - loss: 0.1815\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8750 - loss: 0.1583\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8750 - loss: 0.1349\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8750 - loss: 0.1138\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8750 - loss: 0.0971\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8750 - loss: 0.0838\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8750 - loss: 0.0716\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8750 - loss: 0.0607\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8750 - loss: 0.0517\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8750 - loss: 0.0443\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.8750 - loss: 0.0379\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8750 - loss: 0.0324\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8750 - loss: 0.0279\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8750 - loss: 0.0244\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8750 - loss: 0.0218\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8750 - loss: 0.0199\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8750 - loss: 0.0182\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8750 - loss: 0.0165\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8750 - loss: 0.0148\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8750 - loss: 0.0132\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8750 - loss: 0.0119\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8750 - loss: 0.0110\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8750 - loss: 0.0103\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8750 - loss: 0.0097\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8750 - loss: 0.0091\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8750 - loss: 0.0085\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8750 - loss: 0.0079\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8750 - loss: 0.0073\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8750 - loss: 0.0069\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8750 - loss: 0.0065\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8750 - loss: 0.0062\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8750 - loss: 0.0059\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8750 - loss: 0.0057\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8750 - loss: 0.0054\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8750 - loss: 0.0052\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8750 - loss: 0.0049\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8750 - loss: 0.0047\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8750 - loss: 0.0045\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8750 - loss: 0.0043\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8750 - loss: 0.0042\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8750 - loss: 0.0040\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8750 - loss: 0.0039\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8750 - loss: 0.0038\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8750 - loss: 0.0036\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8750 - loss: 0.0035\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8750 - loss: 0.0034\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8750 - loss: 0.0033\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8750 - loss: 0.0032\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8750 - loss: 0.0031\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8750 - loss: 0.0030\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8750 - loss: 0.0030\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8750 - loss: 0.0029\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8750 - loss: 0.0028\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8750 - loss: 0.0028\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8750 - loss: 0.0027\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8750 - loss: 0.0026\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8750 - loss: 0.0026\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8750 - loss: 0.0025\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8750 - loss: 0.0025\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8750 - loss: 0.0024\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8750 - loss: 0.0024\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n",
      "நீங்கள் எப்படி இருக்கிறீர்கள்\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "english_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I love deep learning.\",\n",
    "    \"What is your name?\",\n",
    "    \"Where do you live?\"\n",
    "]\n",
    "\n",
    "tamil_sentences = [\n",
    "    \"வணக்கம், நீங்கள் எப்படி இருக்கிறீர்கள்?\",\n",
    "    \"எனக்கு ஆழ்ந்த கற்றல் பிடிக்கும்.\",\n",
    "    \"உங்கள் பெயர் என்ன?\",\n",
    "    \"நீங்கள் எங்கு வாழ்கிறீர்கள்?\"\n",
    "]\n",
    "\n",
    "# Tokenize English sentences\n",
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(english_sentences)\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_sequences = eng_tokenizer.texts_to_sequences(english_sentences)\n",
    "\n",
    "# Tokenize Tamil sentences\n",
    "tam_tokenizer = Tokenizer()\n",
    "tam_tokenizer.fit_on_texts(tamil_sentences)\n",
    "tam_vocab_size = len(tam_tokenizer.word_index) + 1\n",
    "tam_sequences = tam_tokenizer.texts_to_sequences(tamil_sentences)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_length = max(max(len(seq) for seq in eng_sequences), max(len(seq) for seq in tam_sequences))\n",
    "eng_sequences = pad_sequences(eng_sequences, maxlen=max_length, padding='post')\n",
    "tam_sequences = pad_sequences(tam_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Split input-output pairs\n",
    "X_train = eng_sequences\n",
    "y_train = tam_sequences\n",
    "\n",
    "# Define Seq2Seq Model (Encoder-Decoder)\n",
    "embedding_dim = 128\n",
    "units = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = keras.layers.Input(shape=(max_length,))\n",
    "enc_emb = keras.layers.Embedding(eng_vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm = keras.layers.LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = keras.layers.Input(shape=(max_length,))\n",
    "dec_emb = keras.layers.Embedding(tam_vocab_size, embedding_dim, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = keras.layers.LSTM(units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(tam_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Compile Model\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Shift decoder input (y_train) correctly\n",
    "decoder_input_data = y_train[:, :-1]  # Remove last token\n",
    "decoder_target_data = y_train[:, 1:]  # Remove first token\n",
    "\n",
    "# Ensure shapes match\n",
    "print(\"Encoder Input Shape:\", X_train.shape)  # (num_samples, max_length)\n",
    "print(\"Decoder Input Shape:\", decoder_input_data.shape)  # (num_samples, max_length - 1)\n",
    "print(\"Decoder Target Shape:\", decoder_target_data.shape)  # (num_samples, max_length - 1)\n",
    "\n",
    "# Fix shape mismatch by adjusting max_length\n",
    "decoder_input_data = pad_sequences(decoder_input_data, maxlen=max_length, padding='post')\n",
    "decoder_target_data = pad_sequences(decoder_target_data, maxlen=max_length, padding='post')\n",
    "\n",
    "# Train Model\n",
    "model.fit([X_train, decoder_input_data], decoder_target_data, batch_size=64, epochs=100)\n",
    "\n",
    "\n",
    "# Function to translate English → Tamil\n",
    "def translate(sentence):\n",
    "    sequence = eng_tokenizer.texts_to_sequences([sentence])\n",
    "    sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    states_value = model.predict([sequence, np.zeros((1, max_length))])\n",
    "    predicted_seq = np.argmax(states_value, axis=-1)[0]\n",
    "    output_sentence = ' '.join([word for word, index in tam_tokenizer.word_index.items() if index in predicted_seq])\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "நீங்கள் எப்படி இருக்கிறீர்கள்\n"
     ]
    }
   ],
   "source": [
    "# Test Translation\n",
    "print(translate(\"Hello, how are you?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq data - ICU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Input\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Load Dataset (Assume it has time-series ICU data)\n",
    "# Columns: ['patient_id', 'time', 'heart_rate', 'blood_pressure', 'oxygen_level', 'outcome']\n",
    "df = pd.read_csv(\"icu_timeseries_data.csv\")\n",
    "\n",
    "# Sort by patient ID and time\n",
    "df = df.sort_values(by=['patient_id', 'time'])\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "df[['heart_rate', 'blood_pressure', 'oxygen_level']] = scaler.fit_transform(df[['heart_rate', 'blood_pressure', 'oxygen_level']])\n",
    "\n",
    "# Encode categorical target (e.g., mortality outcome: 0 = survived, 1 = deceased)\n",
    "label_encoder = LabelEncoder()\n",
    "df['outcome'] = label_encoder.fit_transform(df['outcome'])\n",
    "\n",
    "# Convert to time-series sequences\n",
    "sequence_length = 10  # Use past 10 time steps to predict outcome\n",
    "features = ['heart_rate', 'blood_pressure', 'oxygen_level']\n",
    "patient_groups = df.groupby('patient_id')\n",
    "\n",
    "X, y = [], []\n",
    "for _, group in patient_groups:\n",
    "    data = group[features].values\n",
    "    labels = group['outcome'].values\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(labels[i + sequence_length]) \n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM Model\n",
    "model = Sequential([\n",
    "    Input(shape=(sequence_length, len(features))),  # Input shape: (10, 3)\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification (mortality prediction)\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Evaluate Model\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Performance Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"AUC Score: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6921\n",
      "Epoch 2/10, Loss: 0.6891\n",
      "Epoch 3/10, Loss: 0.6898\n",
      "Epoch 4/10, Loss: 0.6888\n",
      "Epoch 5/10, Loss: 0.6891\n",
      "Epoch 6/10, Loss: 0.6892\n",
      "Epoch 7/10, Loss: 0.6888\n",
      "Epoch 8/10, Loss: 0.6891\n",
      "Epoch 9/10, Loss: 0.6884\n",
      "Epoch 10/10, Loss: 0.6898\n",
      "Test Accuracy: 49.50%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Simulated ICU dataset (for demonstration purposes)\n",
    "def generate_synthetic_icu_data(num_samples=1000, time_steps=10, num_features=5):\n",
    "    X = np.random.rand(num_samples, time_steps, num_features).astype(np.float32)  # Vital signs\n",
    "    y = np.random.randint(0, 2, size=(num_samples, 1)).astype(np.float32)  # Binary outcome (e.g., survival vs. mortality)\n",
    "    return X, y\n",
    "\n",
    "# Prepare data\n",
    "X, y = generate_synthetic_icu_data()\n",
    "X_train, y_train = torch.tensor(X[:800]), torch.tensor(y[:800])\n",
    "X_test, y_test = torch.tensor(X[800:]), torch.tensor(y[800:])\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Define LSTM model\n",
    "class ICU_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout=0.3):\n",
    "        super(ICU_LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        output = self.fc(lstm_out[:, -1, :])  # Take last time step's output\n",
    "        return self.sigmoid(output)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 5  # Number of features\n",
    "hidden_dim = 64\n",
    "output_dim = 1  # Binary classification\n",
    "num_layers = 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ICU_LSTM(input_dim, hidden_dim, output_dim, num_layers).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            predicted = (y_pred > 0.5).float()\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# video LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, output_dir, fps=10):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_rate = int(cap.get(cv2.CAP_PROP_FPS)) // fps\n",
    "    count = 0\n",
    "    frame_id = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % frame_rate == 0:\n",
    "            cv2.imwrite(f\"{output_dir}/frame_{frame_id}.jpg\", frame)\n",
    "            frame_id += 1\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {frame_id} frames.\")\n",
    "\n",
    "# Example usage\n",
    "extract_frames(\"example_video.mp4\", \"frames\", fps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load ResNet model\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*(list(resnet.children())[:-1]))  # Remove final layer\n",
    "resnet.eval()\n",
    "\n",
    "# Define transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def extract_features(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = resnet(img).squeeze().numpy()\n",
    "    return features\n",
    "\n",
    "# Extract features from all frames\n",
    "frame_features = []\n",
    "for frame_file in sorted(os.listdir(\"frames\")):\n",
    "    frame_path = os.path.join(\"frames\", frame_file)\n",
    "    feature_vector = extract_features(frame_path)\n",
    "    frame_features.append(feature_vector)\n",
    "\n",
    "frame_features = np.array(frame_features)  # Shape: (num_frames, feature_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class VideoLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=2048, hidden_dim=512, num_layers=2, num_classes=10):\n",
    "        super(VideoLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Take last LSTM output\n",
    "        return out\n",
    "\n",
    "# Initialize model\n",
    "model = VideoLSTM(input_dim=2048, hidden_dim=512, num_layers=2, num_classes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Convert features to tensor\n",
    "X_train = torch.tensor(frame_features, dtype=torch.float32).unsqueeze(0)  # (batch, seq, feature)\n",
    "y_train = torch.tensor([0])  # Example label\n",
    "\n",
    "# Define optimizer and loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_train)\n",
    "    predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "    print(f\"Predicted class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, output_dir, fps=10):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_rate = int(cap.get(cv2.CAP_PROP_FPS)) // fps\n",
    "    count = 0\n",
    "    frame_id = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % frame_rate == 0:\n",
    "            frame_path = os.path.join(output_dir, f\"frame_{frame_id}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            frame_id += 1\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {frame_id} frames.\")\n",
    "\n",
    "# Example usage\n",
    "extract_frames(\"example_video.mp4\", \"frames\", fps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained ResNet model\n",
    "resnet = ResNet50(weights=\"imagenet\", include_top=False, pooling=\"avg\")\n",
    "\n",
    "def extract_features(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    features = resnet.predict(img_array)\n",
    "    return features.flatten()\n",
    "\n",
    "# Extract features from all frames\n",
    "frame_features = []\n",
    "for frame_file in sorted(os.listdir(\"frames\")):\n",
    "    frame_path = os.path.join(\"frames\", frame_file)\n",
    "    feature_vector = extract_features(frame_path)\n",
    "    frame_features.append(feature_vector)\n",
    "\n",
    "frame_features = np.array(frame_features)  # Shape: (num_frames, feature_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Define LSTM model\n",
    "def create_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        LSTM(512, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.5),\n",
    "        LSTM(256),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define input shape (sequence_length, feature_dim)\n",
    "input_shape = (frame_features.shape[0], frame_features.shape[1])\n",
    "num_classes = 5  # Example number of categories\n",
    "\n",
    "# Create model\n",
    "model = create_lstm_model(input_shape, num_classes)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to TensorFlow tensors\n",
    "X_train = np.expand_dims(frame_features, axis=0)  # (batch, seq, feature)\n",
    "y_train = np.array([[1, 0, 0, 0, 0]])  # Example one-hot encoded label\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict video class\n",
    "y_pred = model.predict(X_train)\n",
    "predicted_class = np.argmax(y_pred, axis=1)[0]\n",
    "print(f\"Predicted Class: {predicted_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
